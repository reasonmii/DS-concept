### Basic Concept
- Mean : the average value in a dataset.
- Median : the middle value in a dataset.
- Mode : the most frequently occurring value in a dataset.
- Range : the difference between the largest and smallest value in a dataset.
- Variance : the average of the squared difference of each data point from the mean. 
- Standard deviation : how spread out your values are from the mean of your dataset.
  - $s = \sqrt{\frac{\sum (x - \bar{x})^2}{n - 1}}$
- Percentile : the value below which a percentage of data falls. 
- Quartile divides the values in a dataset into four equal parts.
  - Q1 : 25th percentile, Q2 : 50th percentile, Q3 : 75th percentile
- Interquartile range (IQR) : Q3 - Q1
  - the distance between the first quartile (Q1) and the third quartile (Q3)

### The probability of multiple events
- Mutually exclusive events : If 2 events cannot occur at the same time.
- Independent events : If the occurrence of one event does not change the probability of the other event.
- 3 basic rules
  - Complement rule : P(A') = 1 - P(A)
    - the probability that event A does not occur is 1 minus the probability of A
  - Addition rule : P(A or B) = P(A) + P(B)
    - if events A and B are mutually exclusive
    - ex) die roll : eitehr 2 or 4
  - Multiplication rule : P(A and B) = P(A) $\times$ P(B)
    - if events A and B are independent
- Conditional Probability
  - two events are dependent if the occurrence of one event changes the probability of the other event. 
  - P(A and B) = P(A) $\times$ P(B|A)
  - P(B|A) = P(A and B) / P(A)
- Calculate conditional probability with Bayes's theorem
  - $P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}$
    - P(A|B) : Posterior probability
    - P(B|A) : Likelihood
    - P(A) : Prior probability
    - P(B) : Evidence
  - Posterior = Likelihood * Prior / Evidence

### Discrete probability distributions
- Uniform Distribution
  - events whose outcomes are all equally likely, or have equal probability
- Binomial Distribution
  - the probability of events with only two possible outcomes: success or failure.
  - This definition assumes the following
    - Each event is independent, or does not affect the probability of the others.
    - Each event has the same probability of success. 
  - Use case
    - A new medication generates side effects
    - A credit card transaction is fraudulent
    - A stock price rises in value 
- Bernoulli Distribution
  - similar to the binomial distribution
    - as it also models events that have only two possible outcomes (success or failure)
  - The only difference is that the Bernoulli distribution refers to only a single trial of an experiment,
    - while the binomial refers to repeated trials.
  - A classic example of a Bernoulli trial is a single coin toss.
- Poisson Distribution
  - the probability that a certain number of events will occur during a specific time period. 
  - Use case
    - Calls per hour for a customer service call center
    - Customers per day at a shop
    - Thunderstorms per month in a city
    - Financial transactions per second at a bank

### Model data with the normal distribution
- Probability functions
  - Probability Mass Functions (PMFs) represent discrete random variables
  - Probability Density Functions (PDFs) represent continuous random variables 
- Normal Distribution
  - The shape is a bell curve
  - The mean is located at the center of the curve
  - The curve is symmetrical on both sides of the mean
  - The total area under the curve equals 1
- The Empirical rule
  - 68% of values fall within 1 standard deviation of the mean
  - 95% of values fall within 2 standard deviations of the mean
  - 99.7% of values fall within 3 standard deviations of the mean

### Sampling
- Population : the entire dataset that you want to draw conclusions about. 
- Sample : a subset of a population. 
- Sampling : the process of selecting a subset of data from a population.
  - Identify the target population
  - Select the sampling frame
  - Choose the sampling method
  - Determine the sample size
  - Collect the sample data
- Why Sampling?
  - It’s often impossible or impractical to collect data on the whole population due to size, complexity, or lack of accessibility
  - It’s easier, faster, and more efficient to collect data from a sample
  - Using a sample saves money and resources
  - Storing, organizing, and analyzing smaller datasets is usually easier, faster, and more reliable than dealing with extremely large datasets 
- Probability Sampling method
  - Simple random sampling
    - every member of a population is selected randomly and has an equal chance of being chosen
  - Stratified random sampling
    - divide a population into groups, and randomly select some members from each group to be in the sample
  - Cluster random sampling
    - divide a population into clusters, randomly select certain clusters, and include all members from the chosen clusters in the sample. 
  - Systematic random sampling
    - put every member of a population into an ordered sequence.
    - Then, you choose a random starting point in the sequence and select members for your sample at regular intervals. 
- Non-probability Sampling Methods 
  - Convenience sampling
    - choose members of a population that are easy to contact or reach
  - Voluntary response sampling
    - consists of members of a population who volunteer to participate in a study.
  - Snowball sampling
    - recruit initial participants to be in a study and then ask them to recruit other people to participate in the study. 
  - Purposive sampling
    - select participants based on the purpose of their study. 
    - applicants who do not fit the profile are rejected. 
- Central Limit Theorem
  - It states that the sampling distribution of the mean approaches a **normal distribution** as the sample size increases. 
  - In order to apply the central limit theorem, the following conditions must be met
    - Randomization
      - Your sample data must be the result of random selection.
      - Random selection means that every member in the population has an equal chance of being chosen for the sample.
    - Independence
      - Your sample values must be independent of each other.
      - Independence means that the value of one observation does not affect the value of another observation.
    - Sample size: The sample size needs to be sufficiently large.
    - Requirements for precision.
      - The larger the sample size,
      - the more closely your sampling distribution will resemble a normal distribution,
      - and the more precise your estimate of the population mean will be.
    - The shape of the population.
      - If your population distribution is roughly bell-shaped and already resembles a normal distribution,
      - the sampling distribution of the sample mean will be close to a normal distribution even with a small sample size. 
- Sampling distribution : a probability distribution of a sample statistic
  - Standard Error : standard deviation of a **sample** statistic

### Confidence Intervals
- Confidence level expresses the uncertainty of the estimation process
  - long-term success rate of the method, or the estimation process based on random sampling. 
- 95% confidence
  - if you take repeated random samples from a population,
  - and construct a confidence interval for each sample using the same method,
  - you can expect that 95% of these intervals will capture the population mean.
  - You can also expect that 5% of the total will not capture the population mean. 
- Incorrect interpretations
  - 95% refers to the probability that the population mean falls within the constructed interval
    - Remember that a 95% confidence level refers to the success rate of the estimation process.
  - 95% refers to the percentage of data values that fall within the interval 
    - This is not necessarily true.
    - A 95% confidence interval shows a range of values that likely includes the actual population mean.
- Construct a confidence interval
  - Identify a sample statistic.
  - Choose a confidence level.
    - Large sample: Z-scores - calculate the margin of error
    - Small sample: T-scores
      - use the t-distribution
      - because there is more uncertainty involved in estimating the standard error for small sample sizes
  - Find the margin of error.
    - For a small sample size, you calculate the margin of error by multiplying the t-score by the standard error.
    - standard error (means) = $SE(x) = s / \sqrt{n}$
  - Calculate the interval.
    - Confidence intervals for small sample sizes only deal with population means, and not population proportions. 

### One-tailed and Two-tailed tests
- A one-tailed test results when the alternative hypothesis states that the actual value of a population parameter is either **less than or greater than** the value in the null hypothesis. 
  - A left-tailed test : the actual value of the parameter **<** the value in the null hypothesis
  - A right-tailed test : the actual value of the parameter **>** the value in the null hypothesis. 
- A two-tailed test results when the alternative hypothesis states that the actual value of the parameter **does not equal** the value in the null hypothesis.
- In general, a one-tailed test may provide more power to detect an effect in a single direction
  - However, before conducting a one-tailed test, you should consider the consequences of missing an effect in the other direction. 
- Example
  - the null hypothesis states that the mean weight of a penguin population equals 30 lbs. 
  - left-tailed test : the alternative hypothesis states that the mean weight of the penguin population is less than (“<“) 30 lbs. 
  - right-tailed test : the alternative hypothesis states that the mean weight of the penguin population is greater than (“>“) 30 lbs. 
  - two-tailed test : the alternative hypothesis states that the mean weight of the penguin population is not equal to (“!=“) 30 lbs. 

### A/B Testing
- Compare two versions of something to find out which version performs better
- Randomized controlled experiment 
  - test subjects are randomly assigned to a control group and a treatment group
  - Treatment : new change being tested in the experiment
  - The control group is not exposed to the treatment.
  - The treatment group is exposed to the treatment.
  - The difference in metric values between the two groups measures the treatment’s effect on the test subjects.

